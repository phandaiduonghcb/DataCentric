name: baseline
state: train # train/test/debug
seed: 6789

work_dir: ${hydra:runtime.cwd}

logging:
  root: ./logs


defaults:
  - model: model
  - _self_

dataset:
  train_data_dir: /DataCentric/data/train
  val_data_dir: /DataCentric/data/val
  test_data_dir: /DataCentric/data/val
  train_augment: False
  val_augment: False
  test_augment: False
  crop_size: 64 #224 #32 #384
  batch_size: 16
  num_workers: 4
  pin_memory: True
  
  augmentation:
    aug0:
      _target_: torchvision.transforms.RandomResizedCrop
      size: ${dataset.crop_size}
      scale:
        - 0.49
        - 1.0
    aug1:
      _target_: torchvision.transforms.RandomHorizontalFlip
      p: 0.5
    aug2:
      _target_: torchvision.transforms.GaussianBlur
      kernel_size: 3
      sigma:
        - 0.2
        - 2.0
    aug3:
      _target_: torchvision.transforms.RandomVerticalFlip
      p: 0.2
    aug4:
      _target_: torchvision.transforms.ColorJitter
      brightness: 0.0
      contrast: 0.2
      saturation: 0.0
      hue: 0.0
trainer:
  # GPU related
  precision: 16
  accelerator: gpu
  devices: 1
  num_nodes: 1
  strategy: null #ddp if we want to use Multi-GPUs
  benchmark: True
  sync_batchnorm: False
  # Training related
  # max_steps: 100000
  max_epochs: 10
  # limit_train_batches: 1.0
#   gradient_clip_val: 0.1 # gradient clipping max norm
#   gradient_clip_algorithm: "norm"

# Logging, progress bar
refresh_rate: 10

ddp_plugin:
  # These two args only work with accelerator = "ddp"
  find_unused_params: True # FIXME: Find out why turn this to False will fail to launch the training
  fp16_hook: True
  static_graph: False

model_ckpt:
  dirpath: ckpts/
  filename: "checkpoint-epoch{epoch}-step{step}-val_acc{val/acc:.3f}-val_loss{val/loss:.3f}"
  monitor: ${model.monitor}
  save_last: True
  save_top_k: 1
  mode: min
  auto_insert_metric_name: False

test:
  hydra_run_dir: ./outputs/s
  wandb_saved_checkpoint: 'phandaiduonghcb/zalo_2022/model-gcxlde9b:v10'
  saved_checkpoint_path: '/DataCentric/trained_models/baseline.ckpt'

hydra:
  run:
    dir: ${test.hydra_run_dir}
  # sweep:
  #   dir: ./multirun/${name}-${model.name}
  #   subdir: ${now:%Y-%m-%d-%H-%M-%S}
  # sweeper:
  #   params:
  #     model.optimizer.lr: 1e-5,3e-5,5e-5,1e-6
  #     model.lr_scheduler.T_0: 200,300,400,600
  #     model.lr_scheduler.eta_min: 1e-7,3e-7,5e-7